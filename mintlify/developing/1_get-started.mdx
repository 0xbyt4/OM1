---
title: Installation Guide
description: "Learn how to install, set up and configure OM1."
---

## System Requirements

### Operating System

- Linux (Ubuntu 20, 22, 24)
- MacOS 12.0+

### Hardware

- Sufficient memory to run vision and other models
- Reliable WiFi or other networking
- Sensors such as cameras, microphones, LIDAR units, IMUs
- Actuators and outputs such as speakers, visual displays, and movement platforms (legs, arms, hands)
- Hardware connected to the "central" computer via `Zenoh`, `CycloneDDS`, serial, usb, or custom APIs/libraries

### Software

Ensure you have the following installed on your machine:

- `Python` >= 3.10
- `uv` >= 0.6.2 as the Python package manager and virtual environment
- `portaudio` for audio input and output
- `ffmpeg` for video processing
- Get your OpenMind API key [here](https://portal.openmind.org/)

#### UV (A Rust and Python package manager)

```bash
# Mac
brew install uv

# Linux
curl -LsSf https://astral.sh/uv/install.sh | sh
```

#### PortAudio Library

For audio functionality, install `portaudio`:

```bash
# Mac
brew install portaudio

# Linux
sudo apt-get update
sudo apt-get install portaudio19-dev
```

Note: `python-all-dev` may also be needed.

#### ffmpeg

For video functionality, install FFmpeg:

```bash Mac
# Mac
brew install ffmpeg

# Linux
sudo apt-get update
sudo apt-get install ffmpeg
```
## CLI

OM1 provides a powerful command-line interface (CLI). The main command is `om1` which provides comprehensive agent management:

```bash
# Using uv run
uv run om1 <command> [options]

# Or activate venv first, then use directly
source .venv/bin/activate
om1 <command> [options]
```

**Core Commands:**
- `om1 run <config>`: Start an agent
- `om1 stop <config>`: Stop an agent
- `om1 status`: Show running agents
- `om1 list`: List available configurations
- `om1 doctor`: Check system requirements
- `om1 setup`: Interactive setup wizard

For complete CLI documentation, see [CLI Reference](./10_cli).

## Installation and Setup

### 1. Clone the repository

```bash
git clone https://github.com/OpenMind/OM1.git
cd OM1
git submodule update --init
```

### 2. Install dependencies

```bash
# Minimal install (~500MB) - for local LLM (Ollama)
uv sync

# Or install with specific extras as needed
uv sync --extra vision      # For vision/face detection
uv sync --extra twitter     # For Twitter integration
uv sync --extra robotics    # For robot hardware
uv sync --extra full        # Everything
```

### 3. Run setup wizard (recommended for first-time users)

```bash
uv run om1 setup
```

This will guide you through:
- Choosing a configuration
- Installing required dependencies
- Setting up environment variables
- Validating your setup

### 4. Or configure manually

Create a `.env` file from the template:

```bash
cp .env.example .env
```

Edit `.env` and add your API keys:

```bash
# Required for most configs
OM_API_KEY=om1_live_...

# For OpenAI-based configs
OPENAI_API_KEY=sk-...
```

Get your OpenMind API key at https://portal.openmind.org/

### 5. Verify setup

```bash
uv run om1 doctor
```

### 6. Run an agent

```bash
# List available configurations
uv run om1 list

# Run an agent (e.g., local LLM)
uv run om1 run ollama

# Or run Spot agent
uv run om1 run spot
```

**Note**: Agent configuration names are only required when switching between different agents. Once an agent has been run, it becomes the default for subsequent executions.

Spot is just an example agent configuration.

If you want to interact with the agent and see how it works, make sure ASR and TTS are configured in `spot.json5`.

ASR configuration (check in agent_inputs)
```bash
{
      "type": "GoogleASRInput"
}
```

TTS configuration (check in agent_actions)
```bash
{
      name: "speak",
      llm_label: "speak",
      connector: "elevenlabs_tts",
      config:
      {
        voice_id: "i4CzbCVWoqvD0P1QJCUL",
        "silence_rate": 20,
      },
}
```

During the first execution, the system will automatically resolve and install all project dependencies. This process may take several minutes to complete before the agent becomes operational.

**Runtime Configuration**

Upon successful initialization, a `.runtime.json5` file will be generated in the `config/memory` directory. This file serves as a snapshot of the agent configuration used in the current session.

**Subsequent Executions**

After the initial run, you can start the agent using the simplified command:

```bash
uv run om1 run
```

![Reference](../assets/hot_reload.png)

The system will automatically load the most recent agent configuration from memory. Additionally, a `.runtime.json5` file will be created in the root config directory, which persists across sessions unless a different agent configuration is specified.

**Switching Agent Configurations**

To run a different agent (for example, the conversation agent), specify the configuration name explicitly:

```bash
uv run om1 run conversation
```

### WebSim to check input and output

Go to [http://localhost:8000](http://localhost:8000) to see real time logs along with the input and output in the terminal. For easy debugging, add `--debug` to see additional logging information.

### Understanding the Log Data

The log data provide insight into how the `spot` agent makes sense of its environment and decides on its next actions.

  - First, it detects a person using vision.
  - Communicates with an external AI API for response generation.
  - The LLM(s) decide on a set of actions (dancing and speaking).
  - The simulated robot expresses emotions via a front-facing display.
  - Logs latency and processing times to monitor system performance.

```bash
Object Detector INPUT
// START
You see a person in front of you. You also see a laptop.
// END

AVAILABLE ACTIONS:
command: move
    A movement to be performed by the agent.
    Effect: Allows the agent to move.
    Arguments: Allowed values: 'stand still', 'sit', 'dance', 'shake paw', 'walk', 'walk back', 'run', 'jump', 'wag tail'

command: speak
    Words to be spoken by the agent.
    Effect: Allows the agent to speak.
    Arguments: <class 'str'>

command: emotion
    A facial expression to be performed by the agent.
    Effect: Performs a given facial expression.
    Arguments: Allowed values: 'cry', 'smile', 'frown', 'think', 'joy'

What will you do? Command:

INFO:httpx:HTTP Request: POST https://api.openmind.org/api/core/openai/chat/completions "HTTP/1.1 200 OK"
INFO:root:OpenAI LLM output: commands=[Command(type='move', value='wag tail'), Command(type='speak', value="Hi there! I see you and I'm excited!"), Command(type='emotion', value='joy')]
```

## More Examples

There are more pre-configured agents in the `/config` folder. They can be run with the following command:

For example, to run the `cubly` agent:

```bash
uv run om1 run cubly
```

If you configure a custom agent, replace `<agent_name>` with your agent and run the below command:

```bash
uv run om1 run <agent_name>
```

## Managing Running Agents

```bash
# Check running agents
uv run om1 status

# Stop a specific agent
uv run om1 stop cubly

# Stop all agents
uv run om1 stop --all

# Restart an agent
uv run om1 restart cubly
```

To get started with development, refer [here](https://docs.openmind.org/developer_cookbook/introduction)
